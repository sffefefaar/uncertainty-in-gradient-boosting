{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import argparse\n",
    "import sys\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "def write(X, y, file):\n",
    "    with open(file, \"w\") as f:\n",
    "        for i in range(len(y)):\n",
    "            f.write(\"{y}\\t{x}\\n\".format(y=y[i], x='\\t'.join(map(str, X[i]))))\n",
    "def writeOOD(X, Y, file):\n",
    "    y = []\n",
    "    with open(file, \"w\") as f:\n",
    "        for i in range(len(X)):\n",
    "            y.append(0)\n",
    "            f.write(\"{y}\\t{x}\\n\".format(y=0, x='\\t'.join(map(str, X[i]))))\n",
    "        for i in range(len(Y)):\n",
    "            y.append(1)\n",
    "            f.write(\"{y}\\t{x}\\n\".format(y=1, x='\\t'.join(map(str, Y[i]))))\n",
    "    return np.array(y)\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "def Auc(x, y):\n",
    "    order = np.lexsort((y, x))\n",
    "    x, y = np.array(x)[order], np.array(y)[order]\n",
    "    return auc(x, y)\n",
    "\n",
    "def reject_MSE(targets, preds, measure, measure_name, save_path, pos_label=1, show=True):\n",
    "    if pos_label != 1:\n",
    "        measure_loc = -1.0 * measure\n",
    "    else:\n",
    "        measure_loc = measure\n",
    "    preds = np.squeeze(preds)\n",
    "    # Compute total MSE\n",
    "    error = np.array(list(map(int, (2 * np.array(targets) - 1) *  ( 2 * np.array(preds) - 1) < 0)))\n",
    "    MSE_0 = np.mean(error)\n",
    "    # print 'BASE MSE', MSE_0\n",
    "\n",
    "    # Create array\n",
    "    array = np.concatenate(\n",
    "        (preds[:, np.newaxis], targets[:, np.newaxis], error[:, np.newaxis], measure_loc[:, np.newaxis]), axis=1)\n",
    "\n",
    "    # Results arrays\n",
    "    results_max = [[0.0, 0.0]]\n",
    "    results_var = [[0.0, 0.0]]\n",
    "    results_min = [[0.0, 0.0]]\n",
    "\n",
    "    optimal_ranking = array[:, 2].argsort()\n",
    "    sorted_array = array[optimal_ranking]  # Sort by error\n",
    "\n",
    "    for i in range(1, array.shape[0]):\n",
    "        x = np.concatenate((sorted_array[:-i, 0], sorted_array[-i:, 1]), axis=0)\n",
    "        mse = np.mean((2 * x - 1) * (2 * sorted_array[:, 1] - 1) < 0)\n",
    "        # Best rejection\n",
    "        results_max.append([float(i) / float(array.shape[0]), (MSE_0 - mse) / MSE_0])\n",
    "        # Random Rejection\n",
    "        results_min.append([float(i) / float(array.shape[0]), float(i) / float(array.shape[0])])\n",
    "\n",
    "    uncertainty_ranking = array[:, 3].argsort()\n",
    "    sorted_array = array[uncertainty_ranking]  # Sort by uncertainty\n",
    "\n",
    "    for i in range(1, array.shape[0]):\n",
    "        x = np.concatenate((sorted_array[:-i, 0], sorted_array[-i:, 1]), axis=0)\n",
    "        mse = np.mean((2 * x - 1) * (2 * sorted_array[:, 1] - 1) < 0)\n",
    "        results_var.append([float(i) / float(array.shape[0]), (MSE_0 - mse) / MSE_0])\n",
    "\n",
    "    max_auc = Auc([x[0] for x in results_max], [x[1] for x in results_max])\n",
    "    var_auc = Auc([x[0] for x in results_var], [x[1] for x in results_var])\n",
    "    min_auc = Auc([x[0] for x in results_min], [x[1] for x in results_min])\n",
    "\n",
    "#     plt.scatter([x[0] for x in results_max], [x for x in np.asarray(sorted(measure_loc, reverse=True))])\n",
    "#     plt.xlim(0.0, 1.0)\n",
    "#     if show == True:\n",
    "#         plt.savefig(os.path.join(save_path, measure_name), bbox_inches='tight')\n",
    "#     plt.close()\n",
    "#     plt.plot([x[0] for x in results_max], [x[1] for x in results_max], '^',\n",
    "#              [x[0] for x in results_var], [x[1] for x in results_var], 'o',\n",
    "#              [x[0] for x in results_min], [x[1] for x in results_min], '--')\n",
    "#     plt.legend(['Optimal-Rejection', 'Model-Rejection', 'Expected Random-Rejection'], loc=4)\n",
    "#     plt.xlim(0.0, 1.0)\n",
    "#     plt.ylim(0.0, 1.0)\n",
    "#     plt.xlabel('Rejection Fraction')\n",
    "#     plt.ylabel('Pearson Correlation')\n",
    "#     if show==True:\n",
    "#         plt.savefig(os.path.join(save_path, \"MSE Rejection Curve using \" + measure_name), bbox_inches='tight')\n",
    "#     plt.close()\n",
    "\n",
    "    AUC_RR = (var_auc - min_auc) / (max_auc - min_auc)\n",
    "#     if save_path is not None:\n",
    "#         with open(os.path.join(save_path, 'results.txt'), 'a') as f:\n",
    "#             f.write('MSE ROC using ' + measure_name + \": \" + str(np.round(AUC_RR * 100.0, 1)) + '\\n')\n",
    "    return error, AUC_RR\n",
    "\n",
    "def reject_MSE1(targets, preds, measure, measure_name, save_path, pos_label=1, show=True):\n",
    "    return roc_auc_score(targets, measure)\n",
    "def run_exp(dataset, num, num_ensembles, langevin, params_dict=None):\n",
    "    approach = \"sglb\"\n",
    "    results_dict = dict()\n",
    "    for data_directory in [dataset]:\n",
    "        from subprocess import call\n",
    "\n",
    "        _DATA_DIRECTORY_PATH = \"./\" + data_directory + \"/\"\n",
    "        _DATA_FILE_OOD = _DATA_DIRECTORY_PATH + \"test-mix-ood.tsv\"\n",
    "        _DATA_TRAIN = _DATA_DIRECTORY_PATH + \"train.tsv\"\n",
    "        _DATA_TRAIN_WITHOUT_VALIDATION = _DATA_DIRECTORY_PATH + \"train_without_validation.tsv\"\n",
    "        _DATA_VALIDATION = _DATA_DIRECTORY_PATH + \"validation.tsv\"\n",
    "        _DATA_TEST = _DATA_DIRECTORY_PATH + \"test.tsv\"\n",
    "\n",
    "\n",
    "        # We fix the random seed\n",
    "\n",
    "        np.random.seed(1)\n",
    "\n",
    "        print (\"Loading data and other hyperparameters...\")\n",
    "        # We load the data\n",
    "        label = list(sorted([int(x[0]) for x in [x.strip().split('\\t') for x in open(_DATA_DIRECTORY_PATH + \"pool.cd\").readlines()] if x[1][0].upper() in ['T', 'L']]))[0]\n",
    "        data = pd.read_csv(_DATA_TRAIN, sep='\\t', header=None)\n",
    "        data_without_validation = pd.read_csv(_DATA_TRAIN_WITHOUT_VALIDATION, sep='\\t', header=None)\n",
    "        data_test = pd.read_csv(_DATA_TEST, sep='\\t', header=None)\n",
    "        data_validation = pd.read_csv(_DATA_VALIDATION, sep='\\t', header=None)\n",
    "        data_ood = pd.read_csv(_DATA_FILE_OOD, sep='\\t', header=None)\n",
    "\n",
    "        y_train = data[ label ].values\n",
    "        y_train_w = data_without_validation[label].values\n",
    "        y_validation = data_validation[label].values\n",
    "        y_test = data_test[label].values\n",
    "        for i in [data, data_without_validation, data_test, data_validation, data_ood]:\n",
    "            i.drop(columns=[label], inplace=True)\n",
    "        Xood = data_ood.values\n",
    "        X_train = data.values\n",
    "        X_train_w = data_without_validation.values\n",
    "        X_test = data_test.values\n",
    "        X_validation = data_validation.values\n",
    "        poolcd = list(sorted([(int(x[0]) + int(int(x[0]) < label), x[1]) for x in [x.strip().split('\\t') for x in open(_DATA_DIRECTORY_PATH + \"pool.cd\").readlines()] if x[1][0].upper() not in ['T', 'L']]))\n",
    "        poolcd = [(0, \"Label\")] + poolcd\n",
    "        with open(\"_pool.cd\", \"w\") as out:\n",
    "            for k, v in poolcd:\n",
    "                out.write(\"{}\\t{}\\n\".format(k, v))\n",
    "        \n",
    "        # We iterate over the training test splits\n",
    "\n",
    "        n_splits = 1\n",
    "        print (\"Done.\")\n",
    "\n",
    "        errors, MC_errors, lls = [], [], []\n",
    "        for split in range(int(n_splits)):\n",
    "            print(\"split {} of {}\".format(split, n_splits))\n",
    "            # We load the indexes of the training and test sets\n",
    "\n",
    "            X_train_original = X_train\n",
    "            y_train_original = y_train\n",
    "            X_train = X_train_w\n",
    "            y_train = y_train_w\n",
    "            y_train_mean = y_train.mean()\n",
    "            y_train_std = y_train.std(ddof=0)\n",
    "            y_train_normalized = y_train\n",
    "            y_validate_normalized = y_validation\n",
    "            write(X_train, y_train_normalized, \"_train.tsv\")\n",
    "            write(X_validation, y_validate_normalized, \"_validation.tsv\")\n",
    "            N = len(y_train)\n",
    "            beta = (N + 0.) * int(langevin)\n",
    "            shrinkage = (0.5 / N) * int(langevin)\n",
    "            # Printing the size of the training, validation and test sets\n",
    "            print ('Number of training examples: ' + str(X_train.shape[0]))\n",
    "            print ('Number of validation examples: ' + str(X_validation.shape[0]))\n",
    "            print ('Number of test examples: ' + str(X_test.shape[0]))\n",
    "            print ('Number of train_original examples: ' + str(X_train_original.shape[0]))\n",
    "\n",
    "            # List of hyperparameters which we will try out using grid-search\n",
    "            depths = [4,5,6]\n",
    "            lrs = [0.0001, 0.001, 0.01, 0.1]\n",
    "            rss = [0]\n",
    "            best_depth = 4\n",
    "            best_lr = 0.003\n",
    "            best_rs = 0.1\n",
    "            iterations = [1000]\n",
    "            best_ll = float(\"-inf\")\n",
    "            best_iters = 1000\n",
    "            total = len(depths) * len(lrs) * len(rss) * len(iterations)\n",
    "            count = 0\n",
    "            if params_dict is None or len(params_dict.keys()) == 0:\n",
    "                params_dict = {}\n",
    "                for depth in depths:\n",
    "                    for lr in lrs:\n",
    "                        for rs in rss:\n",
    "                            for iters in iterations:\n",
    "                                count += 1\n",
    "                                print('count {} of total {}'.format(count, total))\n",
    "                                scores = []\n",
    "                                #iterations_ = []\n",
    "                                predictions = []\n",
    "                                for index in range(1):\n",
    "                                    !./ngboost fit -f _train.tsv -t _validation.tsv  --bootstrap-type Bernoulli --subsample 0.5  --cd _pool.cd --use-best-model False --depth $depth --leaf-estimation-backtracking No --learning-rate $lr --random-strength $rs --leaf-estimation-method Gradient --loss-function Logloss --iterations $iters --seed $index -m out_.m --langevin $langevin --diffusion-temperature $beta --model-shrink-rate $shrinkage  | grep best > output__.tsv\n",
    "                                    with open(\"output__.tsv\") as f:\n",
    "                                        lines = f.readlines()\n",
    "                                        scores = [float(line.split('test: ')[-1].split()[0]) for line in lines[-2-num*100:-2]]\n",
    "                                        #iterations_.append(int(lines[-1].split('=')[-1]))\n",
    "\n",
    "                                arr = (np.array(scores)[-1:] if num == 1 else np.array(scores)[99::100])\n",
    "                                ll = -np.mean(arr)\n",
    "                                print(-ll)\n",
    "                                if ll > best_ll:\n",
    "                                    best_ll = ll\n",
    "                                    best_rs = rs\n",
    "                                    best_lr = lr\n",
    "                                    best_depth = depth\n",
    "                                    best_iters = iters#int(np.mean(iterations_) + 0.5)\n",
    "                params_dict[\"depth\"] = best_depth\n",
    "                params_dict[\"rs\"] = best_rs\n",
    "                params_dict[\"lr\"] = best_lr\n",
    "                params_dict[\"iters\"] = best_iters\n",
    "            best_depth = params_dict[\"depth\"]\n",
    "            best_lr = params_dict[\"lr\"]\n",
    "            best_rs = params_dict[\"rs\"]\n",
    "            best_iters = params_dict[\"iters\"]\n",
    "            X_train = X_train_original\n",
    "            y_train = y_train_original\n",
    "            y_train_normalized = y_train\n",
    "            y_test_normalized = y_test\n",
    "            write(X_train, y_train, \"_train.tsv\")\n",
    "            write(X_test, y_test, \"_test.tsv\")\n",
    "            yood = writeOOD(X_test, Xood, \"_ood.tsv\")\n",
    "            N = len(y_train)\n",
    "            beta = (N + 0.) * int(langevin)\n",
    "            shrinkage = 0.5 / N * int(langevin)\n",
    "            def sigm(x):\n",
    "                return (1.0 / (1.0 + 1.0 / np.exp(np.float128(x)))) if x > 0 else (1.0 / (1.0 + np.exp(-np.float128(x))))\n",
    "            predictions = []\n",
    "            for index in range(num_ensembles):\n",
    "                !./ngboost fit -f _train.tsv -t _test.tsv --cd _pool.cd  --bootstrap-type Bernoulli --subsample 0.5 --use-best-model False --depth $best_depth --leaf-estimation-backtracking No --learning-rate $best_lr --random-strength $best_rs --leaf-estimation-method Gradient --loss-function Logloss --iterations $best_iters --seed $index -m out_.m --langevin $langevin --diffusion-temperature $beta --model-shrink-rate $shrinkage > output__.tsv\n",
    "                for i in range(num):\n",
    "                    trees = best_iters - i*100\n",
    "                    !./ngboost calc -m out_.m --input-path _ood.tsv -o out_.tsv --cd _pool.cd --prediction-type RawFormulaVal --tree-count-limit $trees > stdoutapply.txt\n",
    "                    with open(\"out_.tsv\") as f:\n",
    "                        tmp = [x.strip().split('\\t') for x in f.readlines()[1:]]\n",
    "                        CC = (1 - best_lr * shrinkage) ** (-i*100)\n",
    "                        predictions.append([sigm(CC * float(x[1])) for x in tmp])\n",
    "            lls_ = []\n",
    "            for index in range(len(y_test)):\n",
    "                p = 0.0\n",
    "                yy = y_test[index]\n",
    "                for i in range(max(num, num_ensembles)):\n",
    "                    p += predictions[i][index]/max(num, num_ensembles)\n",
    "                lls_.append(- max(0, yy) * np.log(p) - (1-max(0, yy)) * np.log(1-p))\n",
    "            ll = np.mean(lls_)\n",
    "            preds = np.array([np.mean([x[y] for x in predictions]) for y in range(len(y_test))])\n",
    "            total = np.array([-np.log(x) * x - np.log(1- x) * ( 1- x) for x in np.array([np.mean([x[y] for x in predictions]) for y in range(len(y_test))])])\n",
    "            _, total_score = reject_MSE(y_test_normalized, preds, total, \"variance\", \"anywhere\")\n",
    "            ent = np.array([np.mean(-np.log(x) * x - np.log(1- x) * ( 1- x)) for x in np.array([[x[y] for x in predictions] for y in range(len(y_test))])])\n",
    "            _, ent_score = reject_MSE(y_test_normalized, preds, ent, \"variance\", \"anywhere\")\n",
    "            knowledge = total - ent\n",
    "            acc, knowledge_score = reject_MSE(y_test_normalized, preds, knowledge, \"variance\", \"anywhere\")\n",
    "\n",
    "            y_test = yood\n",
    "            y_test_normalized = yood\n",
    "            preds = np.array([np.mean([x[y] for x in predictions]) for y in range(len(y_test))])\n",
    "            total = np.array([-np.log(x) * x - np.log(1- x) * ( 1- x) for x in np.array([np.mean([x[y] for x in predictions]) for y in range(len(y_test))])])\n",
    "            total_score1 = reject_MSE1(y_test_normalized, preds, total, \"variance\", \"anywhere\")\n",
    "            ent = np.array([np.mean(-np.log(x) * x - np.log(1- x) * ( 1- x)) for x in np.array([[x[y] for x in predictions] for y in range(len(y_test))])])\n",
    "            ent_score1 = reject_MSE1(y_test_normalized, preds, ent, \"variance\", \"anywhere\")\n",
    "            knowledge = total - ent\n",
    "            knowledge_score1 = reject_MSE1(y_test_normalized, preds, knowledge, \"variance\", \"anywhere\")\n",
    "            print(ll)\n",
    "            errors.append(acc)\n",
    "            MC_errors.append([total_score, ent_score, knowledge_score, total_score1, ent_score1, knowledge_score1])\n",
    "            print([total_score, ent_score, knowledge_score, total_score1, ent_score1, knowledge_score1])\n",
    "            lls.append(lls_)\n",
    "        results_dict[data_directory] = (errors[0], MC_errors, lls[0])\n",
    "    return results_dict, {data_directory: params_dict}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = {\"SGB\": {}, \"SGLB\": {}, \"VSGLB\": {}}\n",
    "results = {\"SGB\": {}, \"SGLB\": {}}\n",
    "results[\"SGB-10\"] = {}\n",
    "results[\"SGLB-10\"] = {}\n",
    "results[\"VSGLB-10\"] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult\n",
      "Loading data and other hyperparameters...\n",
      "Done.\n",
      "split 0 of 1\n",
      "Number of training examples: 31259\n",
      "Number of validation examples: 7815\n",
      "Number of test examples: 9768\n",
      "Number of train_original examples: 39074\n",
      "0.27596216843124756024\n",
      "[0.7226481747992137, 0.7226481747992137, 0.08540378445430337, 0.19533603432559207, 0.19533603432559207, 0.5]\n",
      "Loading data and other hyperparameters...\n",
      "Done.\n",
      "split 0 of 1\n",
      "Number of training examples: 31259\n",
      "Number of validation examples: 7815\n",
      "Number of test examples: 9768\n",
      "Number of train_original examples: 39074\n",
      "0.27638441865605534477\n",
      "[0.7153806895860783, 0.7153806895860783, 0.09426848368975427, 0.14169160644897746, 0.14169160644897746, 0.5]\n",
      "Loading data and other hyperparameters...\n",
      "Done.\n",
      "split 0 of 1\n",
      "Number of training examples: 31259\n",
      "Number of validation examples: 7815\n",
      "Number of test examples: 9768\n",
      "Number of train_original examples: 39074\n",
      "0.2754777766774294198\n",
      "[0.7191427097442404, 0.7189409577739091, 0.5282412480536495, 0.19682625863522668, 0.1927834063122761, 0.6673427951069228]\n",
      "Loading data and other hyperparameters...\n",
      "Done.\n",
      "split 0 of 1\n",
      "Number of training examples: 31259\n",
      "Number of validation examples: 7815\n",
      "Number of test examples: 9768\n",
      "Number of train_original examples: 39074\n",
      "0.27556940334588327204\n",
      "[0.7219726088775753, 0.7221787132006474, 0.520672869322074, 0.21311771342483873, 0.20908766847034663, 0.7209697202786884]\n",
      "Loading data and other hyperparameters...\n",
      "Done.\n",
      "split 0 of 1\n",
      "Number of training examples: 31259\n",
      "Number of validation examples: 7815\n",
      "Number of test examples: 9768\n",
      "Number of train_original examples: 39074\n",
      "0.2766790097180641518\n",
      "[0.7145495588376589, 0.7147793575405527, 0.26183718807174255, 0.2183016792199839, 0.21787153190715844, 0.4481180211370629]\n",
      "amazon\n",
      "Loading data and other hyperparameters...\n",
      "Done.\n",
      "split 0 of 1\n",
      "Number of training examples: 20972\n",
      "Number of validation examples: 5243\n",
      "Number of test examples: 6554\n",
      "Number of train_original examples: 26215\n",
      "0.14550888803737521378\n",
      "[0.6564262285960417, 0.6564262285960417, 0.13024743793284696, 0.8274465760733688, 0.8274465760733688, 0.5]\n",
      "Loading data and other hyperparameters...\n",
      "Done.\n",
      "split 0 of 1\n",
      "Number of training examples: 20972\n",
      "Number of validation examples: 5243\n",
      "Number of test examples: 6554\n",
      "Number of train_original examples: 26215\n",
      "0.1429578492256116989\n",
      "[0.6794939276993962, 0.6794939276993962, 0.13423542561428703, 0.8460582749566576, 0.8460582749566576, 0.5]\n",
      "Loading data and other hyperparameters...\n",
      "Done.\n",
      "split 0 of 1\n",
      "Number of training examples: 20972\n",
      "Number of validation examples: 5243\n",
      "Number of test examples: 6554\n",
      "Number of train_original examples: 26215\n",
      "0.14425691387485067975\n",
      "[0.6596896775809762, 0.6599609953523904, 0.5381582192747502, 0.8298905346509864, 0.8302992738451999, 0.580425770614968]\n",
      "Loading data and other hyperparameters...\n",
      "Done.\n",
      "split 0 of 1\n",
      "Number of training examples: 20972\n",
      "Number of validation examples: 5243\n",
      "Number of test examples: 6554\n",
      "Number of train_original examples: 26215\n",
      "0.14319795634980352457\n",
      "[0.6729705860175182, 0.6727858475809014, 0.5990273967166168, 0.8407931496503361, 0.841154895015133, 0.6576242900624958]\n",
      "Loading data and other hyperparameters...\n",
      "Done.\n",
      "split 0 of 1\n",
      "Number of training examples: 20972\n",
      "Number of validation examples: 5243\n",
      "Number of test examples: 6554\n",
      "Number of train_original examples: 26215\n",
      "0.14144803235394944713\n",
      "[0.6920733202161157, 0.6922288755847458, 0.5168117683695848, 0.845675477846671, 0.8456584780966838, 0.7965208637698721]\n",
      "click\n",
      "Loading data and other hyperparameters...\n",
      "Done.\n",
      "split 0 of 1\n",
      "Number of training examples: 255393\n",
      "Number of validation examples: 64193\n",
      "Number of test examples: 79896\n",
      "Number of train_original examples: 319586\n",
      "0.3925685664741690422\n",
      "[0.4357882657955891, 0.4357882657955891, 0.1670547551950792, 0.40981261646949885, 0.40981261646949885, 0.5]\n",
      "Loading data and other hyperparameters...\n",
      "Done.\n",
      "split 0 of 1\n",
      "Number of training examples: 255393\n",
      "Number of validation examples: 64193\n",
      "Number of test examples: 79896\n",
      "Number of train_original examples: 319586\n",
      "0.3926211471772404876\n",
      "[0.43592103035865726, 0.43592103035865726, 0.16762343888556008, 0.3941270058593444, 0.3941270058593444, 0.5]\n",
      "Loading data and other hyperparameters...\n",
      "Done.\n",
      "split 0 of 1\n",
      "Number of training examples: 255393\n",
      "Number of validation examples: 64193\n",
      "Number of test examples: 79896\n",
      "Number of train_original examples: 319586\n",
      "0.3920678338127958531\n",
      "[0.43567106114857085, 0.43562893030119776, 0.25074921508327436, 0.40592004204193793, 0.40200803932114854, 0.8775706133127766]\n",
      "Loading data and other hyperparameters...\n",
      "Done.\n",
      "split 0 of 1\n",
      "Number of training examples: 255393\n",
      "Number of validation examples: 64193\n",
      "Number of test examples: 79896\n",
      "Number of train_original examples: 319586\n",
      "0.39225386567254575914\n",
      "[0.4365263632980488, 0.43643782662407854, 0.2550740554269832, 0.4088355677941553, 0.4052754735030577, 0.8743418781203389]\n",
      "Loading data and other hyperparameters...\n",
      "Done.\n",
      "split 0 of 1\n",
      "Number of training examples: 255393\n",
      "Number of validation examples: 64193\n",
      "Number of test examples: 79896\n",
      "Number of train_original examples: 319586\n",
      "0.39305766098869967707\n",
      "[0.4325719463625085, 0.4326705590294037, 0.10107749304916244, 0.39101098368652254, 0.39006059584679464, 0.7829124439196609]\n",
      "KDDCup09_appetency\n",
      "Loading data and other hyperparameters...\n",
      "Done.\n",
      "split 0 of 1\n",
      "Number of training examples: 31954\n",
      "Number of validation examples: 7826\n",
      "Number of test examples: 10220\n",
      "Number of train_original examples: 39780\n",
      "0.07410671917619518145\n",
      "[0.7070872981696124, 0.7070872981696124, -0.005429935155113056, 0.1522947358121331, 0.1522947358121331, 0.5]\n",
      "Loading data and other hyperparameters...\n",
      "Done.\n",
      "split 0 of 1\n",
      "Number of training examples: 31954\n",
      "Number of validation examples: 7826\n",
      "Number of test examples: 10220\n",
      "Number of train_original examples: 39780\n",
      "0.07348165164347791544\n",
      "[0.7201147308385967, 0.7201147308385967, 0.0006760025911877847, 0.19147311154598823, 0.19147311154598823, 0.5]\n",
      "Loading data and other hyperparameters...\n",
      "Done.\n",
      "split 0 of 1\n",
      "Number of training examples: 31954\n",
      "Number of validation examples: 7826\n",
      "Number of test examples: 10220\n",
      "Number of train_original examples: 39780\n",
      "0.073158618998955960446\n",
      "[0.7162194333501778, 0.7162443436732585, 0.6259097646650923, 0.16387428571428572, 0.16235185909980432, 0.5357185518590998]\n",
      "Loading data and other hyperparameters...\n",
      "Done.\n",
      "split 0 of 1\n",
      "Number of training examples: 31954\n",
      "Number of validation examples: 7826\n",
      "Number of test examples: 10220\n",
      "Number of train_original examples: 39780\n",
      "0.073221568441933309465\n",
      "[0.7131359685758035, 0.7131738755891871, 0.643885270411595, 0.19923673189823876, 0.19527753424657532, 0.72464313111546]\n",
      "Loading data and other hyperparameters...\n",
      "Done.\n",
      "split 0 of 1\n",
      "Number of training examples: 31954\n",
      "Number of validation examples: 7826\n",
      "Number of test examples: 10220\n",
      "Number of train_original examples: 39780\n",
      "0.073438240664516289475\n",
      "[0.7147864349409625, 0.7148234564317542, 0.3509473899457366, 0.17993996086105676, 0.17418416829745595, 0.8573135420743639]\n",
      "KDDCup09_churn\n",
      "Loading data and other hyperparameters...\n",
      "Done.\n",
      "split 0 of 1\n",
      "Number of training examples: 31793\n",
      "Number of validation examples: 8103\n",
      "Number of test examples: 10104\n",
      "Number of train_original examples: 39896\n",
      "0.22981999390498114645\n",
      "[0.4999397658664382, 0.4999397658664382, 0.006999364234648385, 0.9604342636579574, 0.9604342636579574, 0.5]\n",
      "Loading data and other hyperparameters...\n",
      "Done.\n",
      "split 0 of 1\n",
      "Number of training examples: 31793\n",
      "Number of validation examples: 8103\n",
      "Number of test examples: 10104\n",
      "Number of train_original examples: 39896\n",
      "0.2296996377088598928\n",
      "[0.496067684745389, 0.496067684745389, 0.00818886253146862, 0.9187106294536816, 0.9187106294536816, 0.5]\n",
      "Loading data and other hyperparameters...\n",
      "Done.\n",
      "split 0 of 1\n",
      "Number of training examples: 31793\n",
      "Number of validation examples: 8103\n",
      "Number of test examples: 10104\n",
      "Number of train_original examples: 39896\n",
      "0.22773824666926971235\n",
      "[0.5076311317016048, 0.5079141862933712, 0.3811409998587739, 0.9791640538400632, 0.9694237331749803, 0.9982513855898654]\n",
      "Loading data and other hyperparameters...\n",
      "Done.\n",
      "split 0 of 1\n",
      "Number of training examples: 31793\n",
      "Number of validation examples: 8103\n",
      "Number of test examples: 10104\n",
      "Number of train_original examples: 39896\n",
      "0.22889806894349251458\n",
      "[0.5048120619692318, 0.505143633420619, 0.3607396898877607, 0.9660088677751385, 0.9539943784639747, 0.9985975851148061]\n",
      "Loading data and other hyperparameters...\n",
      "Done.\n",
      "split 0 of 1\n",
      "Number of training examples: 31793\n",
      "Number of validation examples: 8103\n",
      "Number of test examples: 10104\n",
      "Number of train_original examples: 39896\n",
      "0.22959154974355712879\n",
      "[0.49828342039763673, 0.49837607219363145, 0.22261679818552305, 0.9497803246239113, 0.944686737925574, 0.9899753958828187]\n",
      "KDDCup09_upselling\n",
      "Loading data and other hyperparameters...\n",
      "Done.\n",
      "split 0 of 1\n",
      "Number of training examples: 32164\n",
      "Number of validation examples: 8013\n",
      "Number of test examples: 9823\n",
      "Number of train_original examples: 40177\n",
      "0.16715131498616159804\n",
      "[0.5445888351388843, 0.5445888351388843, 0.035794789888605336, 0.35456090807289015, 0.35456090807289015, 0.5]\n",
      "Loading data and other hyperparameters...\n",
      "Done.\n",
      "split 0 of 1\n",
      "Number of training examples: 32164\n",
      "Number of validation examples: 8013\n",
      "Number of test examples: 9823\n",
      "Number of train_original examples: 40177\n",
      "0.16338419689380150976\n",
      "[0.5606810161293383, 0.5606810161293383, 0.034359133869871855, 0.6108957752214192, 0.6108957752214192, 0.5]\n",
      "Loading data and other hyperparameters...\n",
      "Done.\n",
      "split 0 of 1\n",
      "Number of training examples: 32164\n",
      "Number of validation examples: 8013\n",
      "Number of test examples: 9823\n",
      "Number of train_original examples: 40177\n",
      "0.16362806992428278862\n",
      "[0.5627642228124472, 0.5626085188318225, 0.5008172087589549, 0.44328184872238624, 0.43418800773694394, 0.8129416267942584]\n",
      "Loading data and other hyperparameters...\n",
      "Done.\n",
      "split 0 of 1\n",
      "Number of training examples: 32164\n",
      "Number of validation examples: 8013\n",
      "Number of test examples: 9823\n",
      "Number of train_original examples: 40177\n",
      "0.16315256910629787967\n",
      "[0.5687032175019964, 0.5689284321882576, 0.4725745456542993, 0.6132635651023108, 0.6059666293393057, 0.9254311513794157]\n",
      "Loading data and other hyperparameters...\n",
      "Done.\n",
      "split 0 of 1\n",
      "Number of training examples: 32164\n",
      "Number of validation examples: 8013\n",
      "Number of test examples: 9823\n",
      "Number of train_original examples: 40177\n",
      "0.16361209686175124471\n",
      "[0.5604787696528737, 0.5605245612427651, 0.17963478913995634, 0.4849730021378398, 0.4818970375648987, 0.8060453425633717]\n",
      "internet\n",
      "Loading data and other hyperparameters...\n",
      "Done.\n",
      "split 0 of 1\n",
      "Number of training examples: 6435\n",
      "Number of validation examples: 1651\n",
      "Number of test examples: 2022\n",
      "Number of train_original examples: 8086\n",
      "0.2283948319289735597\n",
      "[0.7784052367544844, 0.7784052367544844, 0.06734167006925153, 0.7717993164301593, 0.7717993164301593, 0.5]\n",
      "Loading data and other hyperparameters...\n",
      "Done.\n",
      "split 0 of 1\n",
      "Number of training examples: 6435\n",
      "Number of validation examples: 1651\n",
      "Number of test examples: 2022\n",
      "Number of train_original examples: 8086\n",
      "0.22904982866014498035\n",
      "[0.7626476307039682, 0.7626476307039682, 0.08149041937088704, 0.7660545876546112, 0.7660545876546112, 0.5]\n",
      "Loading data and other hyperparameters...\n",
      "Done.\n",
      "split 0 of 1\n",
      "Number of training examples: 6435\n",
      "Number of validation examples: 1651\n",
      "Number of test examples: 2022\n",
      "Number of train_original examples: 8086\n",
      "0.22860991837521549011\n",
      "[0.7690322471599877, 0.7689570763272802, 0.7066565640480665, 0.7685156349462839, 0.7659870141316802, 0.9338807768017157]\n",
      "Loading data and other hyperparameters...\n",
      "Done.\n",
      "split 0 of 1\n",
      "Number of training examples: 6435\n",
      "Number of validation examples: 1651\n",
      "Number of test examples: 2022\n",
      "Number of train_original examples: 8086\n",
      "0.22853089461506750864\n",
      "[0.7621041677563581, 0.7624033735824136, 0.7040147166541947, 0.7701094886936765, 0.7677086699767899, 0.9357982979306834]\n",
      "Loading data and other hyperparameters...\n",
      "Done.\n",
      "split 0 of 1\n",
      "Number of training examples: 6435\n",
      "Number of validation examples: 1651\n",
      "Number of test examples: 2022\n",
      "Number of train_original examples: 8086\n",
      "0.22298246455215908056\n",
      "[0.7816824244356778, 0.7816770550904842, 0.4321273136538644, 0.7117881521089794, 0.7055743259785919, 0.8473812811548218]\n",
      "kick\n",
      "Loading data and other hyperparameters...\n",
      "Done.\n",
      "split 0 of 1\n",
      "Number of training examples: 46654\n",
      "Number of validation examples: 11734\n",
      "Number of test examples: 14595\n",
      "Number of train_original examples: 58388\n",
      "0.2866557953963303322\n",
      "[0.43917729331363864, 0.43917729331363864, 0.006946939842339161, 0.4974618621666892, 0.4974618621666892, 0.5]\n",
      "Loading data and other hyperparameters...\n",
      "Done.\n",
      "split 0 of 1\n",
      "Number of training examples: 46654\n",
      "Number of validation examples: 11734\n",
      "Number of test examples: 14595\n",
      "Number of train_original examples: 58388\n",
      "0.2883592119829786892\n",
      "[0.43418638831931483, 0.43418638831931483, 0.01134385944994239, 0.49009636357781405, 0.49009636357781405, 0.5]\n",
      "Loading data and other hyperparameters...\n",
      "Done.\n",
      "split 0 of 1\n",
      "Number of training examples: 46654\n",
      "Number of validation examples: 11734\n",
      "Number of test examples: 14595\n",
      "Number of train_original examples: 58388\n",
      "0.28549101150742712431\n",
      "[0.44227799427441994, 0.44205298808312615, 0.3468600849024695, 0.698744131187631, 0.6378106847802891, 0.9949402760999908]\n",
      "Loading data and other hyperparameters...\n",
      "Done.\n",
      "split 0 of 1\n",
      "Number of training examples: 46654\n",
      "Number of validation examples: 11734\n",
      "Number of test examples: 14595\n",
      "Number of train_original examples: 58388\n",
      "0.2858361985459363471\n",
      "[0.4453610788219314, 0.44523634550144897, 0.3489753680064843, 0.5438060951622369, 0.49319473599368946, 0.9762399812757554]\n",
      "Loading data and other hyperparameters...\n",
      "Done.\n",
      "split 0 of 1\n",
      "Number of training examples: 46654\n",
      "Number of validation examples: 11734\n",
      "Number of test examples: 14595\n",
      "Number of train_original examples: 58388\n",
      "0.28770491038138373865\n",
      "[0.4340996484413561, 0.4338338105730353, 0.1877456526335843, 0.5836663824794998, 0.572497958255562, 0.9588022022580814]\n"
     ]
    }
   ],
   "source": [
    "#\"adult\", \"amazon\", \"click\", \"KDDCup09_appetency\"\n",
    "first = False\n",
    "for ds in [\"adult\", \"amazon\", \"click\", \"KDDCup09_appetency\", \"KDDCup09_churn\", \"KDDCup09_upselling\", \"internet\", \"kick\"]:\n",
    "    print(ds)\n",
    "    r, p = run_exp(ds, 1, 1, False, params_dict=params[\"SGB\"].get(ds))\n",
    "    results[\"SGB\"].update(r)\n",
    "    params[\"SGB\"].update(p)\n",
    "    r, p = run_exp(ds, 1, 1, True, params_dict=params[\"SGLB\"].get(ds))\n",
    "    results[\"SGLB\"].update(r)\n",
    "    params[\"SGLB\"].update(p)\n",
    "    if not first:\n",
    "        r, p = run_exp(ds, 1, 10, False, params_dict=params[\"SGB\"].get(ds))\n",
    "        results[\"SGB-10\"].update(r)\n",
    "        r, p = run_exp(ds, 1, 10, True, params_dict=params[\"SGLB\"].get(ds))\n",
    "        results[\"SGLB-10\"].update(r)\n",
    "    first = False\n",
    "    r, p = run_exp(ds, 10, 1, True, params_dict=params[\"VSGLB\"].get(ds))\n",
    "    results[\"VSGLB-10\"].update(r)\n",
    "    params[\"VSGLB\"].update(p)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_saved = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "approaches = [(\"SGB-1\", results[\"SGB\"]), (\"SGLB-1\", results[\"SGLB\"]), (\"SGB-10\", results[\"SGB-10\"]), (\"SGLB-10\", results[\"SGLB-10\"]), (\"VSGLB-10\", results[\"VSGLB-10\"])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.6920733202161157,\n",
       "  0.6922288755847458,\n",
       "  0.5168117683695848,\n",
       "  0.845675477846671,\n",
       "  0.8456584780966838,\n",
       "  0.7965208637698721]]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"VSGLB-10\"][\"amazon\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27596216843124756024"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(approaches[0][1][ds][2][:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adult & \\bf 0.128 & \\bf 0.126 & \\bf 0.126 & \\bf 0.127 & \\bf 0.126\\\\\n",
      "Amazon & \\bf 0.044 & \\bf 0.045 & \\bf 0.043 & \\bf 0.044 & \\bf 0.045\\\\\n",
      "Click & \\bf 0.156 & \\bf 0.156 & \\bf 0.156 & \\bf 0.156 & \\bf 0.156\\\\\n",
      "KDDCup09_appetency & \\bf 0.018 & \\bf 0.018 & \\bf 0.018 & \\bf 0.018 & \\bf 0.017\\\\\n",
      "KDDCup09_churn & \\bf 0.071 & \\bf 0.071 & \\bf 0.071 & \\bf 0.071 & \\bf 0.071\\\\\n",
      "KDDCup09_upselling & \\bf 0.047 & \\bf 0.046 & \\bf 0.046 & \\bf 0.046 & \\bf 0.046\\\\\n",
      "Internet &  0.103 & \\bf 0.100 & \\bf 0.101 & \\bf 0.099 & \\bf 0.101\\\\\n",
      "Kick & \\bf 0.095 &  0.095 & \\bf 0.095 & \\bf 0.095 & \\bf 0.095\\\\\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_rel\n",
    "for ds in [\"adult\", \"amazon\", \"click\", \"KDDCup09_appetency\", \"KDDCup09_churn\", \"KDDCup09_upselling\", \"internet\", \"kick\"]:\n",
    "    nll = []\n",
    "    for a in approaches:\n",
    "        nll.append(np.mean(a[1][ds][0]))\n",
    "    maxind = np.argmin(nll)\n",
    "    print(\"{} & {}\\\\\\\\\".format(ds[:1].upper() + ds[1:], \" & \".join(map(lambda x: \"{} {:.3f}\".format(\"\\\\bf\" if x[2] == maxind or ttest_rel(approaches[maxind][1][ds][0], approaches[x[2]][1][ds][0]).pvalue > 0.05 else \"\", int(1000*x[0]) / 1000), zip(nll, stddev, range(len(nll)))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\multirow{3}{*} {Adult}&  \\\\\n",
      " & Total & \\bf {72} & 71 & 71 & \\bf {72} & 71 \\\\\n",
      " & Knowledge & 8 & 9 & 52 & 52 & 26 \\\\ \\midrule\n",
      "\\multirow{3}{*} {Amazon}&  \\\\\n",
      " & Total & 65 & 67 & 65 & 67 & \\bf {69} \\\\\n",
      " & Knowledge & 13 & 13 & 53 & 59 & 51 \\\\ \\midrule\n",
      "\\multirow{3}{*} {Click}&  \\\\\n",
      " & Total & \\bf {43} & \\bf {43} & \\bf {43} & \\bf {43} & \\bf {43} \\\\\n",
      " & Knowledge & 16 & 16 & 25 & 25 & 10 \\\\ \\midrule\n",
      "\\multirow{3}{*} {Appetency}&  \\\\\n",
      " & Total & 70 & \\bf {72} & 71 & 71 & 71 \\\\\n",
      " & Knowledge & 0 & 0 & 62 & 64 & 35 \\\\ \\midrule\n",
      "\\multirow{3}{*} {Churn}&  \\\\\n",
      " & Total & 49 & 49 & \\bf {50} & \\bf {50} & 49 \\\\\n",
      " & Knowledge & 0 & 0 & 38 & 36 & 22 \\\\ \\midrule\n",
      "\\multirow{3}{*} {Upselling}&  \\\\\n",
      " & Total & 54 & \\bf {56} & \\bf {56} & \\bf {56} & \\bf {56} \\\\\n",
      " & Knowledge & 3 & 3 & 50 & 47 & 17 \\\\ \\midrule\n",
      "\\multirow{3}{*} {Internet}&  \\\\\n",
      " & Total & 77 & 76 & 76 & 76 & \\bf {78} \\\\\n",
      " & Knowledge & 6 & 8 & 70 & 70 & 43 \\\\ \\midrule\n",
      "\\multirow{3}{*} {Kick}&  \\\\\n",
      " & Total & 43 & 43 & \\bf {44} & \\bf {44} & 43 \\\\\n",
      " & Knowledge & 0 & 1 & 34 & 34 & 18 \\\\ \\midrule\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_rel\n",
    "def make_ds(x):\n",
    "    return \"\\\\multirow{{{{3}}}}{{{{*}}}} {{{{{}}}}}& {{}} \\\\\\\\ \\\\midrule\".format(x)\n",
    "first = False\n",
    "for ds in [\"adult\", \"amazon\", \"click\", \"KDDCup09_appetency\",\"KDDCup09_churn\", \"KDDCup09_upselling\", \"internet\", \"kick\"]:\n",
    "    ds_ = ds.split(\"KDDCup09_\")[-1]\n",
    "    formatter = make_ds(ds_[:1].upper() + ds_[1:])\n",
    "    data = [a[ds] for _, a in approaches]\n",
    "    results = [[b] for b, _ in approaches] \n",
    "#     unc = list(map(lambda x: x+6, [5, 0, 3, 2, 4, 1]))\n",
    "    unc = list(map(lambda x: x, [0, 2]))\n",
    "    data = [[np.transpose(a[1])[ind] for a in data] for ind in unc]\n",
    "    maxapp = 0\n",
    "    maxind = 0\n",
    "    maxval = -100\n",
    "    for idx, dat in enumerate(data):\n",
    "        arr = list(map(np.mean, dat))\n",
    "        values = max(arr)\n",
    "        ind = np.argmax(arr)\n",
    "        if values > maxval:\n",
    "            maxval = values\n",
    "            maxind = ind\n",
    "            maxapp = idx\n",
    "    for idx, dat in enumerate(data):\n",
    "        values = list(map(np.mean, dat))\n",
    "        ttests = list(map(lambda x: int(100*x[1]) ==  int(100*data[maxapp][maxind]) or maxind == x[0] and maxapp == idx, enumerate(dat)))\n",
    "        stds = np.array(list(map(np.std, dat))) / np.array(list(map(lambda x: np.sqrt(len(x)), dat)))\n",
    "        for mean, std, best, index in zip(values, stds, ttests, range(len(ttests))):\n",
    "            results[index].append(((\"\\\\bf {{{}}}\" if best else \"{}\").format(\"{}\".format(int(100*mean), int(100*std)))))\n",
    "    results_ = results\n",
    "    results = [[] for i in range(len(results_[0]))]\n",
    "    tvs = [\"Total\", \"Knowledge\"]\n",
    "    for i in range(len(tvs)):\n",
    "        results[i+1].append(tvs[i])\n",
    "    for i in range(0 if first else 1, len(results_[0])):\n",
    "        for j in range(len(results_)):\n",
    "            results[i].append(results_[j][i])\n",
    "    results = list(map(lambda x: \" & \".join(x), results))\n",
    "    print(formatter.format(\" \\\\\\\\\\n & \".join(results)))\n",
    "    first = False\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\multirow{3}{*} {Adult}&  \\\\\n",
      " & Total & 19 & 14 & 19 & 21 & 21 \\\\\n",
      " & Data & 19 & 14 & 19 & 20 & 21 \\\\\n",
      " & Knowledge & 50 & 50 & 66 & \\bf {72} & 44 \\\\ \\midrule\n",
      "\\multirow{3}{*} {Amazon}&  \\\\\n",
      " & Total & 82 & \\bf {84} & 82 & \\bf {84} & \\bf {84} \\\\\n",
      " & Data & 82 & \\bf {84} & 83 & \\bf {84} & \\bf {84} \\\\\n",
      " & Knowledge & 50 & 50 & 58 & 65 & 79 \\\\ \\midrule\n",
      "\\multirow{3}{*} {Click}&  \\\\\n",
      " & Total & 40 & 39 & 40 & 40 & 39 \\\\\n",
      " & Data & 40 & 39 & 40 & 40 & 39 \\\\\n",
      " & Knowledge & 50 & 50 & \\bf {87} & \\bf {87} & 78 \\\\ \\midrule\n",
      "\\multirow{3}{*} {Appetency}&  \\\\\n",
      " & Total & 15 & 19 & 16 & 19 & 17 \\\\\n",
      " & Data & 15 & 19 & 16 & 19 & 17 \\\\\n",
      " & Knowledge & 50 & 50 & 53 & 72 & \\bf {85} \\\\ \\midrule\n",
      "\\multirow{3}{*} {Churn}&  \\\\\n",
      " & Total & 96 & 91 & 97 & 96 & 94 \\\\\n",
      " & Data & 96 & 91 & 96 & 95 & 94 \\\\\n",
      " & Knowledge & 50 & 50 & \\bf {99} & \\bf {99} & 98 \\\\ \\midrule\n",
      "\\multirow{3}{*} {Upselling}&  \\\\\n",
      " & Total & 35 & 61 & 44 & 61 & 48 \\\\\n",
      " & Data & 35 & 61 & 43 & 60 & 48 \\\\\n",
      " & Knowledge & 50 & 50 & 81 & \\bf {92} & 80 \\\\ \\midrule\n",
      "\\multirow{3}{*} {Internet}&  \\\\\n",
      " & Total & 77 & 76 & 76 & 77 & 71 \\\\\n",
      " & Data & 77 & 76 & 76 & 76 & 70 \\\\\n",
      " & Knowledge & 50 & 50 & \\bf {93} & \\bf {93} & 84 \\\\ \\midrule\n",
      "\\multirow{3}{*} {Kick}&  \\\\\n",
      " & Total & 49 & 49 & 69 & 54 & 58 \\\\\n",
      " & Data & 49 & 49 & 63 & 49 & 57 \\\\\n",
      " & Knowledge & 50 & 50 & \\bf {99} & 97 & 95 \\\\ \\midrule\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_rel\n",
    "def make_ds(x):\n",
    "    return \"\\\\multirow{{{{3}}}}{{{{*}}}} {{{{{}}}}}& {{}} \\\\\\\\ \\\\midrule\".format(x)\n",
    "first = False\n",
    "for ds in [\"adult\", \"amazon\", \"click\", \"KDDCup09_appetency\",\"KDDCup09_churn\", \"KDDCup09_upselling\", \"internet\", \"kick\"]:\n",
    "    ds_ = ds.split(\"KDDCup09_\")[-1]\n",
    "    formatter = make_ds(ds_[:1].upper() + ds_[1:])\n",
    "    data = [a[ds] for _, a in approaches]\n",
    "    results = [[b] for b, _ in approaches] \n",
    "#     unc = list(map(lambda x: x+6, [5, 0, 3, 2, 4, 1]))\n",
    "    unc = list(map(lambda x: x+3, [0, 1, 2]))\n",
    "    data = [[np.transpose(a[1])[ind] for a in data] for ind in unc]\n",
    "    maxapp = 0\n",
    "    maxind = 0\n",
    "    maxval = -100\n",
    "    for idx, dat in enumerate(data):\n",
    "        arr = list(map(np.mean, dat))\n",
    "        values = max(arr)\n",
    "        ind = np.argmax(arr)\n",
    "        if values > maxval:\n",
    "            maxval = values\n",
    "            maxind = ind\n",
    "            maxapp = idx\n",
    "    for idx, dat in enumerate(data):\n",
    "        values = list(map(np.mean, dat))\n",
    "        ttests = list(map(lambda x: int(100*x[1]) ==  int(100*data[maxapp][maxind]) or maxind == x[0] and maxapp == idx, enumerate(dat)))\n",
    "        stds = np.array(list(map(np.std, dat))) / np.array(list(map(lambda x: np.sqrt(len(x)), dat)))\n",
    "        for mean, std, best, index in zip(values, stds, ttests, range(len(ttests))):\n",
    "            results[index].append(((\"\\\\bf {{{}}}\" if best else \"{}\").format(\"{}\".format(int(100*mean), int(100*std)))))\n",
    "    results_ = results\n",
    "    results = [[] for i in range(len(results_[0]))]\n",
    "    tvs = [\"Total\", \"Data\", \"Knowledge\"]\n",
    "    for i in range(len(tvs)):\n",
    "        results[i+1].append(tvs[i])\n",
    "    for i in range(0 if first else 1, len(results_[0])):\n",
    "        for j in range(len(results_)):\n",
    "            results[i].append(results_[j][i])\n",
    "    results = list(map(lambda x: \" & \".join(x), results))\n",
    "    print(formatter.format(\" \\\\\\\\\\n & \".join(results)))\n",
    "    first = False\n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
